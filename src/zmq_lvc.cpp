/**
 * @file zmq_lvc.cpp
 *
 * Project Clearwater - IMS in the Cloud
 * Copyright (C) 2013  Metaswitch Networks Ltd
 *
 * This program is free software: you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation, either version 3 of the License, or (at your
 * option) any later version, along with the "Special Exception" for use of
 * the program along with SSL, set forth below. This program is distributed
 * in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 * without even the implied warranty of MERCHANTABILITY or FITNESS FOR
 * A PARTICULAR PURPOSE.  See the GNU General Public License for more
 * details. You should have received a copy of the GNU General Public
 * License along with this program.  If not, see
 * <http://www.gnu.org/licenses/>.
 *
 * The author can be reached by email at clearwater@metaswitch.com or by
 * post at Metaswitch Networks Ltd, 100 Church St, Enfield EN2 6BQ, UK
 *
 * Special Exception
 * Metaswitch Networks Ltd  grants you permission to copy, modify,
 * propagate, and distribute a work formed by combining OpenSSL with The
 * Software, or a work derivative of such a combination, even if such
 * copying, modification, propagation, or distribution would otherwise
 * violate the terms of the GPL. You must comply with the GPL in all
 * respects for all of the code used other than OpenSSL.
 * "OpenSSL" means OpenSSL toolkit software distributed by the OpenSSL
 * Project and licensed under the OpenSSL Licenses, or a work based on such
 * software and licensed under the OpenSSL Licenses.
 * "OpenSSL Licenses" means the OpenSSL License and Original SSLeay License
 * under which the OpenSSL Project distributes the OpenSSL toolkit software,
 * as those licenses appear in the file LICENSE-OPENSSL.
 */

#include "zmq_lvc.h"
#include "log.h"

#include <zmq.h>
#include <stdio.h>
#include <string>
#include <assert.h>

/*
 * LastValueCache
 *
 * This class acts as an aggregating proxy for all statistics generated by the product code.
 * Statistics are internally sent over inproc:// connections encapsulated in Subscription
 * envelopes and externally are sent over a tcp:// publishing socket on port 6666.
 *
 * This proxy also caches the last known value for a statistic and re-publishes it when a
 * subscriber registers interest.  This allows a client to poll the last known value easily.
 */
LastValueCache::LastValueCache(int statcount,
                               const std::string *statnames,
                               long poll_timeout_ms) :  //< Poll period in milliseconds
  _statcount(statcount),
  _statnames(statnames),
  _poll_timeout_ms(poll_timeout_ms),
  _terminate(false)
{
  LOG_DEBUG("Initializing statistics aggregator");
  _context = zmq_ctx_new();
  _subscriber = new void *[_statcount];

  // Bind all the sockets first, before we try to connect. This is a
  // limitation of inproc sockets. See
  // http://zguide.zeromq.org/page:all#Unicast-Transports
  // and the thread at
  // http://lists.zeromq.org/pipermail/zeromq-dev/2010-November/008012.html
  // for the issues leading to this design.
  for (int ii = 0; ii < _statcount; ii++)
  {
    std::string statname = _statnames[ii];
    void* publisher = zmq_socket(_context, ZMQ_PUB);
    zmq_bind(publisher, ("inproc://" + statname).c_str());
    _internal_publishers[statname] = publisher;
    LOG_DEBUG("Opened statistics socket inproc://%s", statname.c_str());
  }

  int rc = pthread_create(&_cache_thread,
                          NULL,
                          &last_value_cache_entry_func,
                          (void *)this);

  if (rc < 0)
  {
    LOG_ERROR("Failed to start statistics aggregator, no statistics will be available");
  }
}

LastValueCache::~LastValueCache()
{
  if (_cache_thread)
  {
    _terminate = true;
    pthread_join(_cache_thread, NULL);
  }
  delete[] _subscriber;

  for (int ii = 0; ii < _statcount; ii++)
  {
    std::string statname = _statnames[ii];
    LOG_DEBUG("Unbinding and closing statistics socket inproc://%s", statname.c_str());

    zmq_unbind(_internal_publishers[statname], ("inproc://" + statname).c_str());
    zmq_close(_internal_publishers[statname]);
  }

  zmq_ctx_destroy(_context);
}

/// Get the bound ZMQ publisher socket to use internally when
/// publishing statistics to this cache.  At most one thread may
/// publish to a given statistic at a time.
void* LastValueCache::get_internal_publisher(std::string statname)
{
  assert(_internal_publishers.find(statname) != _internal_publishers.end());
  return _internal_publishers[statname];
}

void LastValueCache::run()
{
  // One for each internal statistic (0.._statcount-1) and one for the publisher.
  zmq_pollitem_t items[_statcount + 1];

  for (int ii = 0; ii < _statcount; ii++)
  {
    _subscriber[ii] = zmq_socket(_context, ZMQ_SUB);
    LOG_DEBUG("Initializing inproc://%s statistic listener", _statnames[ii].c_str());
    zmq_connect(_subscriber[ii], ("inproc://" + _statnames[ii]).c_str());
    zmq_setsockopt(_subscriber[ii], ZMQ_SUBSCRIBE, "", 0);
  }

  _publisher = zmq_socket(_context, ZMQ_XPUB);
  int verbose = 1;
  zmq_setsockopt(_publisher, ZMQ_XPUB_VERBOSE, &verbose, sizeof(verbose));
  LOG_DEBUG("Enabled XPUB_VERBOSE mode");
  zmq_bind(_publisher, "tcp://*:6666");

  while (!_terminate)
  {
    // Reset the poll items
    for (int ii = 0; ii < _statcount; ii++)
    {
      items[ii].socket = _subscriber[ii];
      items[ii].fd = 0;
      items[ii].events = ZMQ_POLLIN;
      items[ii].revents = 0;
    }
    items[_statcount].socket = _publisher;
    items[_statcount].fd = 0;
    items[_statcount].events = ZMQ_POLLIN;
    items[_statcount].revents = 0;

    // Poll for an event
    //LOG_DEBUG("Poll for %d items", _statcount + 1);
    int rc = zmq_poll(items, _statcount + 1, _poll_timeout_ms);
    assert(rc >= 0 || errno == EINTR);

    for (int ii = 0; ii < _statcount; ii++)
    {
      if (items[ii].revents & ZMQ_POLLIN)
      {
        LOG_DEBUG("Update to %s statistic", _statnames[ii].c_str());
        clear_cache(_subscriber[ii]);
        while (1)
        {
          zmq_msg_t message;
          zmq_msg_t *cached_message = (zmq_msg_t *)malloc(sizeof(zmq_msg_t));
          int more;
          size_t more_size = sizeof (more);

          zmq_msg_init(&message);
          zmq_msg_init(cached_message);
          zmq_msg_recv(&message, _subscriber[ii], 0);
          zmq_msg_copy(cached_message, &message);
          _cache[_subscriber[ii]].push_back(cached_message);
          zmq_getsockopt(_subscriber[ii], ZMQ_RCVMORE, &more, &more_size);
          zmq_msg_send(&message, _publisher, more ? ZMQ_SNDMORE : 0);
          zmq_msg_close(&message);
          if (!more)
            break;      //  Last message frame
        }
      }
    }

    // Recognize incoming subscription events
    if (items[_statcount].revents & ZMQ_POLLIN)
    {
      zmq_msg_t message;
      zmq_msg_init(&message);
      zmq_msg_recv(&message, _publisher, 0);
      char *msg_body = (char *)zmq_msg_data(&message);
      if (msg_body[0] == ZMQ_NEW_SUBSCRIPTION_MARKER)
      {
        // This is a new subscription
        std::string topic = std::string(msg_body + 1, zmq_msg_size(&message) - 1);
        LOG_DEBUG("New subscription for %s", topic.c_str());
        bool recognized = false;

        for (int ii = 0; ii < _statcount; ii++)
        {
          if (topic == _statnames[ii])
          {
            LOG_DEBUG("Statistic found, check for cached value");
            recognized = true;

            // Replay the cached message if one exists
            if (_cache.find(_subscriber[ii]) != _cache.end())
            {
              replay_cache(_subscriber[ii]);
            }
            else
            {
              LOG_DEBUG("No cached record found, reporting empty statistic");
              std::string status = "OK";
              zmq_send(_publisher, _statnames[ii].c_str(), _statnames[ii].length(), ZMQ_SNDMORE);
              zmq_send(_publisher, status.c_str(), status.length(), 0);
            }
          }
        }

        if (!recognized)
        {
          LOG_DEBUG("Subscription for unknown stat %s", topic.c_str());
          std::string status = "Unknown";
          zmq_send(_publisher, topic.c_str(), topic.length(), ZMQ_SNDMORE);
          zmq_send(_publisher, status.c_str(), status.length(), 0);
        }
      }
      zmq_msg_close(&message);
    }
  }

  for (int ii = 0; ii < _statcount; ii++)
  {
    zmq_disconnect(_subscriber[ii], ("inproc://" + _statnames[ii]).c_str());
    zmq_close(_subscriber[ii]);
    clear_cache(_subscriber[ii]);
  }
  zmq_unbind(_publisher, "tcp://*:6666");
  zmq_close(_publisher);
}

void LastValueCache::clear_cache(void *entry)
{
  LOG_DEBUG("Clearing message cache for %p", entry);
  if (_cache.find(entry) == _cache.end())
  {
    // Entry not found, add a blank vector in as we're about to fill it.
    _cache[entry] = std::vector<zmq_msg_t *>();
  }
  else
  {
    std::vector<zmq_msg_t *> *msg_list = &_cache[entry];
    std::vector<zmq_msg_t *>::iterator it = msg_list->begin();
    while (it != msg_list->end())
    {
      zmq_msg_t *cached_message = *it;
      zmq_msg_close(cached_message);
      free(cached_message);
      it = msg_list->erase(it);
    }
  }
}

void LastValueCache::replay_cache(void *entry)
{
  std::vector<zmq_msg_t *> *cache_record = &_cache[entry];
  if (cache_record->empty())
  {
    LOG_DEBUG("No cached record");
    return;
  }

  LOG_DEBUG("Replaying cache for entry %p (length: %d)", cache_record, entry, cache_record->size());
  std::vector<zmq_msg_t *>::iterator it;
  for (std::vector<zmq_msg_t *>::iterator it = cache_record->begin();
       it != cache_record->end();
       it++)
  {
    zmq_msg_t message;
    zmq_msg_init(&message);
    zmq_msg_copy(&message, *it);
    zmq_sendmsg(_publisher, &message, (it + 1 != cache_record->end()) ? ZMQ_SNDMORE : 0);
    zmq_msg_close(&message);
  }
}

void* LastValueCache::last_value_cache_entry_func(void *lvc)
{
  ((LastValueCache *)lvc)->run();
  return NULL;
}
